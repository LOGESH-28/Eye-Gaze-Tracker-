# ğŸ¯ Eye Gaze Tracker â€“ Multi-User Real-Time Gaze Tracking System

An intelligent, multi-user gaze tracking system built in Python using OpenCV, MediaPipe, and Tkinter.  
Designed for environments where multiple users interact with a shared screen â€” ideal for research, accessibility, education, and human-computer interaction.

---

## ğŸ§  Overview

This project implements a complete eye-tracking pipeline:

- Face detection and unique identification (without saving biometric data).
- Prompting users to enter their names with a visual of their face.
- A gamified calibration interface using moving targets.
- Real-time gaze tracking with unique color-coded dots and names.
- Persistent user data across sessions using local storage.

All of this is presented through a clean, modern UI with dynamic styling and smooth UX.

---

## ğŸ–¼ï¸ Features

- âœ… **Multi-user detection and registration**
- ğŸ® **Animated, stress-free calibration system**
- ğŸ‘ï¸ **Real-time gaze tracking using smoothed prediction**
- ğŸŸ¢ **Unique colored dots with user names showing gaze focus**
- ğŸ–¥ï¸ **Full-screen transparent overlay canvas**
- ğŸ’¾ **Persistent user info (names, calibration models, color)**
- âš¡ **Live camera preview and FPS tracking**

---

## ğŸ“ Project Structure

```text
â”œâ”€â”€ main_app.py             # Entry point with dependency check and app boot
â”œâ”€â”€ gaze_tracker_ui.py      # Modern Tkinter UI for user management, preview, status
â”œâ”€â”€ gaze_tracker_logic.py   # Core tracking, face detection, calibration, model training
â”œâ”€â”€ user_data.json          # Persistent data for user registry and trained models
â””â”€â”€ README.md               # Project documentation


ğŸ› ï¸ Technologies Used
Python 3.10

OpenCV (cv2) â€“ Video capture, image processing

MediaPipe â€“ Face mesh landmark detection

Scikit-learn â€“ Polynomial + RANSAC regression for gaze prediction

Tkinter â€“ GUI framework with real-time updates and dialogs

Pillow â€“ Image processing for face preview

JSON â€“ Lightweight data persistence

Workflow:
Launch the system â†’ Click â€œStart Trackingâ€

Each detected face is shown â†’ User is prompted to enter a name

Once all users are named â†’ Begin animated gamified calibration

After calibration â†’ Colored, labeled dots track each personâ€™s gaze in real-time


ğŸ§ª Calibration Details
9 randomized screen positions

Animated dot moves to encourage relaxed interaction

Captures multiple samples per user per dot

Uses regression models (Polynomial + RANSAC) to estimate gaze direction

Smooths gaze points using an exponential moving average


ğŸ” Privacy and Ethics
No facial images or biometric data are stored

Face data is used temporarily for registration only

Users are identified by name and session-local IDs
